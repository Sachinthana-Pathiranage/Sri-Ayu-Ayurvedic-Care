# -*- coding: utf-8 -*-
"""Treatment Outcome Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jFk9PCa0ydXsAa-TxGY-7NtMEfUMaKvt
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer

# Load the dataset
file_path = "sri-ayu-backend/dataset/dropdown_dataset.xlsx"

# Check sheet names
xls = pd.ExcelFile(file_path)
xls.sheet_names

# Load the first sheet into a DataFrame
df = pd.read_excel(file_path, sheet_name='Sheet1')

# Display basic information about the dataset
df.info(), df.head()

import re
import numpy as np

# Ensure Recovery_Time is a string and remove spaces
df["Recovery_Time"] = df["Recovery_Time"].astype(str).str.strip()

# Replace non-standard dashes (–, —) with a normal hyphen (-)
df["Recovery_Time"] = df["Recovery_Time"].str.replace(r"[–—]", "-", regex=True)

# Function to extract the numerical recovery time
def convert_recovery_time(value):
    try:
        # Extract all numbers from the string
        numbers = [int(num) for num in re.findall(r'\d+', value)]

        # If two numbers (range), return the mean; otherwise, return the single value
        return np.mean(numbers) if len(numbers) == 2 else (numbers[0] if numbers else np.nan)
    except:
        return np.nan  # Handle errors safely

# Apply the function
df["Recovery_Time"] = df["Recovery_Time"].apply(convert_recovery_time)

# Encode categorical columns
success_mapping = {"Low": 0, "Medium": 1, "High": 2}
df["Success_Category"] = df["Success_Category"].map(success_mapping)

severity_mapping = {"Mild": 0, "Moderate": 1, "Severe": 2}
df["Severity"] = df["Severity"].map(severity_mapping)

# Feature Engineering

## 1. Extracting word count from Symptoms
df["Symptom_Length"] = df["Symptoms"].apply(lambda x: len(str(x).split()))

## 2. TF-IDF Vectorization on Symptoms
vectorizer = TfidfVectorizer(max_features=50)  # Adjust features as needed
symptom_tfidf = vectorizer.fit_transform(df["Symptoms"])
symptom_tfidf_df = pd.DataFrame(symptom_tfidf.toarray(), columns=vectorizer.get_feature_names_out())

# Strip any extra spaces from column names and string values
df.columns = df.columns.str.strip()
df['Treatment (Sinhala Name)'] = df['Treatment (Sinhala Name)'].str.strip()
df['Treatment (English Name)'] = df['Treatment (English Name)'].str.strip()
df['Symptoms'] = df['Symptoms'].str.strip()

print(df.columns)
df.columns = df.columns.str.strip()

## 3. One-Hot Encoding Disease
disease_encoder = OneHotEncoder(sparse_output=False, drop="first")
disease_encoded = disease_encoder.fit_transform(df[["Disease"]])
disease_encoded_df = pd.DataFrame(disease_encoded, columns=disease_encoder.get_feature_names_out(["Disease"]))

## 4. Label Encoding Treatment (English Name)
treatment_encoder = LabelEncoder()
df["Treatment_Encoded"] = treatment_encoder.fit_transform(df["Treatment (English Name)"])

# Combine the new features with the original dataset
df_final = pd.concat([df, symptom_tfidf_df, disease_encoded_df], axis=1)

# Drop unnecessary columns
df_final.drop(columns=["Treatment (Sinhala Name)", "Treatment (English Name)", "Symptoms", "Disease"], inplace=True)

# Save processed data
df_final.to_csv("processed_dataset.csv", index=False)

print("Feature Engineering Completed! Processed data saved as 'processed_dataset.csv'.")

print(df.info())  # Check data types & missing values
print(df.describe())  # Summary stats for numerical columns
print(df.nunique())  # Unique values per column

print(df.isna().sum())

import seaborn as sns
import matplotlib.pyplot as plt # This line should be added if 'plt' is not already defined

# Distribution of numeric columns
sns.histplot(df['Recovery_Time'], kde=True)
plt.title('Distribution of Recovery Time')
plt.show()

sns.histplot(df['Symptom_Length'], kde=True)
plt.title('Distribution of Symptom Length')
plt.show()

# Distribution of categorical columns
sns.countplot(x='Severity', data=df)
plt.title('Severity Distribution')
plt.show()

sns.countplot(x='Success_Category', data=df)
plt.title('Success Category Distribution')
plt.show()

# The 'Disease' column was one-hot encoded and dropped.
# To visualize its distribution, you can use the encoded columns in 'df_final'
# Assuming the one-hot encoded columns are like 'Disease_Disease1', 'Disease_Disease2', etc.
for disease_col in [col for col in df_final.columns if col.startswith('Disease_')]:
    sns.countplot(x=disease_col, data=df_final)
    plt.title(f'{disease_col} Distribution')
    plt.xticks(rotation=45)  # Rotate x-axis labels for better readability
    plt.show()

sns.boxplot(x=df["Recovery_Time"])
plt.title("Outliers in Recovery Time")
plt.show()

# Correlation heatmap for numeric columns
numeric_cols = ['Severity', 'Recovery_Time', 'Success_Category', 'Symptom_Length', 'Treatment_Encoded']
correlation_matrix = df[numeric_cols].corr()

sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Boxplot for Recovery Time vs Severity
sns.boxplot(x='Severity', y='Recovery_Time', data=df)
plt.title('Recovery Time vs Severity')
plt.show()

# Boxplot for Recovery Time vs Symptom Length
sns.boxplot(x='Symptom_Length', y='Recovery_Time', data=df)
plt.title('Recovery Time vs Symptom Length')
plt.show()

# Success Category by Severity
sns.countplot(x='Severity', hue='Success_Category', data=df)
plt.title('Success Category vs Severity')
plt.show()

# Success Category by Disease (using one-hot encoded columns)
disease_cols = [col for col in df_final.columns if col.startswith('Disease_')]

for disease_col in disease_cols:
    plt.figure()  # Create a new figure for each disease
    sns.countplot(x=disease_col, hue='Success_Category', data=df_final)
    plt.title(f'Success Category vs {disease_col}')
    plt.xticks(rotation=45)  # Rotate x-axis labels for readability
    plt.show()

df["Symptom_Length"] = df["Symptoms"].apply(lambda x: len(str(x).split()))

df = pd.get_dummies(df, columns=["Disease"], drop_first=True)

df["Recovery_Group"] = pd.cut(df["Recovery_Time"], bins=[0, 5, 10, 20], labels=["Short", "Medium", "Long"])

# Interaction feature between Severity and Symptom Length
df['Severity_Symptom_Interaction'] = df['Severity'] * df['Symptom_Length']

from sklearn.preprocessing import PolynomialFeatures

# Create polynomial features for Recovery Time
poly = PolynomialFeatures(degree=2, include_bias=False)
recovery_time_poly = poly.fit_transform(df[['Recovery_Time']])

# Add polynomial features to the DataFrame
df_poly = pd.DataFrame(recovery_time_poly, columns=['Recovery_Time_poly1', 'Recovery_Time_poly2'])
df = pd.concat([df, df_poly], axis=1)

# Check the column names in the dataframe
print("Column Names:\n", df.columns)

# Check the first few rows of the dataframe to spot any issues with columns
print("\nFirst few rows of the dataframe:\n", df.head())

# Check the unique values in 'Symptoms' column to identify any strange values
print("\nUnique Symptoms:\n", df['Symptoms'].unique())

"""Feature Importance (Using ML Models)"""

# Step 1: Clean column names
df.columns = df.columns.str.strip()  # Remove leading/trailing spaces

# Step 2: Encode Symptoms - One-hot Encoding for symptoms
df_encoded = pd.get_dummies(df, columns=["Symptoms"], drop_first=True)

# Step 3: Ensure categorical columns are properly encoded
le = LabelEncoder()

# Label Encoding for categorical columns like 'Recovery_Group'
df_encoded['Recovery_Group'] = le.fit_transform(df_encoded['Recovery_Group'])

# Optional: Label Encoding for Treatment columns (if required)
df_encoded['Treatment (Sinhala Name)'] = le.fit_transform(df_encoded['Treatment (Sinhala Name)'])
df_encoded['Treatment (English Name)'] = le.fit_transform(df_encoded['Treatment (English Name)'])

# Step 4: Prepare X and y for regression
X = df_encoded.drop(columns=["Recovery_Time", "Success_Category"])  # Drop target columns
y = df_encoded["Recovery_Time"]  # Target variable

# Step 5: Train the RandomForestRegressor model
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor()
model.fit(X, y)

# Step 6: Plot feature importances
import matplotlib.pyplot as plt

feat_importances = pd.Series(model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh')
plt.title("Top 10 Feature Importances")
plt.show()

import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression, LinearRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
from sklearn.neural_network import MLPClassifier, MLPRegressor
from sklearn.metrics import classification_report, accuracy_score, mean_squared_error, confusion_matrix

# Prepare X and y for both tasks
# Task 1: Predicting Recovery Time (Regression)
X_reg = df_encoded.drop(columns=["Recovery_Time", "Success_Category"])  # Features for regression
y_reg = df_encoded["Recovery_Time"]  # Target variable for regression (Recovery_Time)

# Task 2: Predicting Success Category (Classification)
X_class = df_encoded.drop(columns=["Recovery_Time", "Success_Category"])  # Features for classification
y_class = df_encoded["Success_Category"]  # Target variable for classification (Success_Category)

# Split data into training and testing sets for both tasks
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)
X_train_class, X_test_class, y_train_class, y_test_class = train_test_split(X_class, y_class, test_size=0.3, random_state=42)

# Models for classification (Success_Category)
classification_models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "SVM (Linear)": SVC(kernel='linear'),
    "Random Forest": RandomForestClassifier(n_estimators=100, random_state=42),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(n_estimators=100, random_state=42),
}

# Models for regression (Recovery_Time)
regression_models = {
    "Linear Regression": LinearRegression(),
    "Random Forest Regressor": RandomForestRegressor(n_estimators=100, random_state=42),
    "Gradient Boosting Regressor": GradientBoostingRegressor(n_estimators=100, random_state=42),
}

# Dictionary to hold evaluation results for classification and regression
classification_results = {}
regression_results = {}

# Evaluate classification models
for model_name, model in classification_models.items():
    model.fit(X_train_class, y_train_class)
    y_pred_class = model.predict(X_test_class)

    accuracy = accuracy_score(y_test_class, y_pred_class)
    class_report = classification_report(y_test_class, y_pred_class)
    conf_matrix = confusion_matrix(y_test_class, y_pred_class)
    cross_val_scores = cross_val_score(model, X_class, y_class, cv=5, scoring='accuracy')
    cross_val_mean = cross_val_scores.mean()
    cross_val_std = cross_val_scores.std()

    classification_results[model_name] = {
        "Accuracy": accuracy,
        "Cross-Validation Mean Accuracy": cross_val_mean,
        "Cross-Validation Std Dev": cross_val_std,
        "Classification Report": class_report,
        "Confusion Matrix": conf_matrix
    }

# Evaluate regression models
for model_name, model in regression_models.items():
    model.fit(X_train_reg, y_train_reg)
    y_pred_reg = model.predict(X_test_reg)

    mse = mean_squared_error(y_test_reg, y_pred_reg)
    rmse = mse ** 0.5
    cross_val_scores = cross_val_score(model, X_reg, y_reg, cv=5, scoring='neg_mean_squared_error')
    cross_val_mean = -cross_val_scores.mean()
    cross_val_std = cross_val_scores.std()

    regression_results[model_name] = {
        "MSE": mse,
        "RMSE": rmse,
        "Cross-Validation Mean MSE": cross_val_mean,
        "Cross-Validation Std Dev": cross_val_std
    }

# Print the evaluation results for classification models
print("Classification Results (Success Category):\n")
for model_name, results in classification_results.items():
    print(f"Model: {model_name}")
    print(f"Accuracy: {results['Accuracy']:.4f}")
    print(f"Cross-Validation Mean Accuracy: {results['Cross-Validation Mean Accuracy']:.4f}")
    print(f"Cross-Validation Std Dev: {results['Cross-Validation Std Dev']:.4f}")
    print("Classification Report:\n", results['Classification Report'])
    print("Confusion Matrix:\n", results['Confusion Matrix'])
    print("="*80)

# Print the evaluation results for regression models
print("Regression Results (Recovery Time):\n")
for model_name, results in regression_results.items():
    print(f"Model: {model_name}")
    print(f"MSE: {results['MSE']:.4f}")
    print(f"RMSE: {results['RMSE']:.4f}")
    print(f"Cross-Validation Mean MSE: {results['Cross-Validation Mean MSE']:.4f}")
    print(f"Cross-Validation Std Dev: {results['Cross-Validation Std Dev']:.4f}")
    print("="*80)

from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, Ridge, Lasso, ElasticNet
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.model_selection import train_test_split
import pandas as pd

# Clean the feature names to remove any special characters or spaces
X.columns = X.columns.str.replace('[^a-zA-Z0-9_]', '_', regex=True)

# Define the target variables
y_success_category = df_encoded["Success_Category"]  # Target for classification
y_recovery_time = df_encoded["Recovery_Time"]  # Target for regression

# Define the feature matrix X (without target columns)
X = df_encoded.drop(columns=["Success_Category", "Recovery_Time"])

# Scale features (important for models like Logistic Regression and SVM)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Split data into training and testing sets for both regression and classification
X_train, X_test, y_train_class, y_test_class = train_test_split(X_scaled, y_success_category, test_size=0.2, random_state=42)
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_scaled, y_recovery_time, test_size=0.2, random_state=42)

# Classification models (for Success Category)
classification_models = {
    "Logistic Regression": LogisticRegression(max_iter=1000),
    "SVM (Linear)": SVC(kernel='linear'),
    "Random Forest": RandomForestClassifier(),
    "XGBoost": xgb.XGBClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "KNN": KNeighborsClassifier(),
}

# Regression models (for Recovery Time)
regression_models = {
    "Random Forest Regressor": RandomForestRegressor(),
    "Linear Regression": Ridge(),  # Using Ridge as an example, you can replace with other regressors
    "KNN Regressor": KNeighborsRegressor(),
    "Gradient Boosting Regressor": GradientBoostingRegressor(),
    "XGBoost Regressor": xgb.XGBRegressor(),
}

# Initialize results dictionaries
classification_results = {}
regression_results = {}

# Evaluate classification models for Success Category
for model_name, model in classification_models.items():
    try:
        model.fit(X_train, y_train_class)
        y_pred_class = model.predict(X_test)
        accuracy = accuracy_score(y_test_class, y_pred_class)
        classification_results[model_name] = {'Accuracy': accuracy}
    except Exception as e:
        classification_results[model_name] = {'Error': str(e)}

# Evaluate regression models for Recovery Time
for model_name, model in regression_models.items():
    try:
        model.fit(X_train_reg, y_train_reg)
        y_pred_reg = model.predict(X_test_reg)
        mse = mean_squared_error(y_test_reg, y_pred_reg)
        rmse = mse ** 0.5
        regression_results[model_name] = {'MSE': mse, 'RMSE': rmse}
    except Exception as e:
        regression_results[model_name] = {'Error': str(e)}

# Display classification results
print("Classification Results (Success Category):\n")
for model_name, metrics in classification_results.items():
    print(f"{model_name}: {metrics}")
    print('-' * 50)

# Display regression results
print("\nRegression Results (Recovery Time):\n")
for model_name, metrics in regression_results.items():
    print(f"{model_name}: {metrics}")
    print('-' * 50)

from sklearn.metrics import accuracy_score, mean_squared_error

# Initialize results dictionary for overfitting check
overfitting_results = {}

# Check for overfitting in classification models
for model_name, model in classification_models.items():
    model.fit(X_train, y_train_class)

    # Performance on training set
    y_train_pred_class = model.predict(X_train)
    train_accuracy = accuracy_score(y_train_class, y_train_pred_class)

    # Performance on test set
    y_test_pred_class = model.predict(X_test)
    test_accuracy = accuracy_score(y_test_class, y_test_pred_class)

    # Store the results
    overfitting_results[model_name] = {
        'Train Accuracy': train_accuracy,
        'Test Accuracy': test_accuracy
    }

# Check for overfitting in regression models
for model_name, model in regression_models.items():
    model.fit(X_train_reg, y_train_reg)

    # Performance on training set
    y_train_pred_reg = model.predict(X_train_reg)
    train_mse = mean_squared_error(y_train_reg, y_train_pred_reg)
    train_rmse = train_mse ** 0.5

    # Performance on test set
    y_test_pred_reg = model.predict(X_test_reg)
    test_mse = mean_squared_error(y_test_reg, y_test_pred_reg)
    test_rmse = test_mse ** 0.5

    # Store the results
    overfitting_results[model_name] = {
        'Train MSE': train_mse,
        'Test MSE': test_mse,
        'Train RMSE': train_rmse,
        'Test RMSE': test_rmse
    }

# Display overfitting results
print("Overfitting Check Results:\n")
for model_name, metrics in overfitting_results.items():
    print(f"{model_name}:")
    for metric, value in metrics.items():
        print(f"  {metric}: {value}")
    print('-' * 50)

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.linear_model import LogisticRegression, Ridge, Lasso
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, RandomForestRegressor
from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor
from sklearn.metrics import accuracy_score, mean_squared_error
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectFromModel
import xgboost as xgb

# Clean feature names to remove special characters or spaces
X.columns = X.columns.str.replace('[^a-zA-Z0-9_]', '_', regex=True)

# Define the target variables
y_success_category = df_encoded["Success_Category"]
y_recovery_time = df_encoded["Recovery_Time"]

# Define the feature matrix X (without target columns)
X = df_encoded.drop(columns=["Success_Category", "Recovery_Time"])

# Split data into training and testing sets
X_train, X_test, y_train_class, y_test_class = train_test_split(X, y_success_category, test_size=0.2, random_state=42, stratify=y_success_category)
X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X, y_recovery_time, test_size=0.2, random_state=42)

# Standardize the data for models that are sensitive to scaling
scaler = RobustScaler()  # Better for handling outliers
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Feature Selection using XGBoost
xgb_feature_selector = xgb.XGBClassifier(n_estimators=100, max_depth=3)
xgb_feature_selector.fit(X_train_scaled, y_train_class)
selector = SelectFromModel(xgb_feature_selector, prefit=True)
X_train_selected = selector.transform(X_train_scaled)
X_test_selected = selector.transform(X_test_scaled)

# PCA for dimensionality reduction (keeping 80% variance)
pca = PCA(n_components=0.8)
X_train_pca = pca.fit_transform(X_train_selected)
X_test_pca = pca.transform(X_test_selected)

# Apply Gaussian Noise (Dropout-style regularization)
def add_noise(X, noise_level=0.01):
    return X + noise_level * np.random.normal(size=X.shape)

X_train_pca = add_noise(X_train_pca)
X_test_pca = add_noise(X_test_pca)

# Hyperparameter tuning for Random Forest, XGBoost, and GradientBoosting
rf_params = {
    'n_estimators': [50, 100],  # Reduce trees to avoid overfitting
    'max_depth': [3, 6],  # Reduce tree depth
    'min_samples_split': [5, 10],  # Avoid small splits
    'min_samples_leaf': [2, 5]  # Avoid too small leaves
}

xgb_params = {
    'n_estimators': [50, 100],
    'learning_rate': [0.01, 0.05],  # Reduce learning rate
    'max_depth': [2, 4],  # Less depth = less overfitting
    'subsample': [0.7, 0.8],  # Reduce overfitting
    'gamma': [1, 5],  # Penalize complexity
    'lambda': [1, 10],  # L2 regularization
    'min_child_weight': [3, 5]
}

ridge_params = {'alpha': [10, 100, 500]}  # Stronger regularization

# GridSearchCV for hyperparameter tuning
cv_strategy = StratifiedKFold(n_splits=15)

rf_grid_search = GridSearchCV(RandomForestClassifier(), rf_params, cv=cv_strategy)
xgb_grid_search = GridSearchCV(xgb.XGBClassifier(), xgb_params, cv=cv_strategy)

# Fit the classifiers with hyperparameter tuning
rf_grid_search.fit(X_train_pca, y_train_class)
xgb_grid_search.fit(X_train_pca, y_train_class)

# Get the best model after GridSearchCV
best_rf = rf_grid_search.best_estimator_
best_xgb = xgb_grid_search.best_estimator_

# Classification models
classification_models = {
    "Logistic Regression": LogisticRegression(max_iter=1000, penalty='l2', C=0.01),  # More regularization
    "SVM (Linear)": SVC(kernel='linear', C=0.01),  # More regularization
    "Random Forest": best_rf,
    "XGBoost": best_xgb,
    "KNN": KNeighborsClassifier(n_neighbors=7),  # Increase K to smooth decision boundary
}

# Regression models
regression_models = {
    "Random Forest Regressor": RandomForestRegressor(n_estimators=50, max_depth=5),
    "Linear Regression": Ridge(alpha=100),  # Increased alpha to reduce overfitting
    "KNN Regressor": KNeighborsRegressor(n_neighbors=7),
    "XGBoost Regressor": xgb.XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.05, subsample=0.7, gamma=5),
}

# Evaluate classification models
classification_results = []
for model_name, model in classification_models.items():
    model.fit(X_train_pca, y_train_class)
    y_pred_train_class = model.predict(X_train_pca)
    y_pred_test_class = model.predict(X_test_pca)

    # Train & Validation Accuracy
    train_accuracy = accuracy_score(y_train_class, y_pred_train_class)
    validation_accuracy = accuracy_score(y_test_class, y_pred_test_class)

    classification_results.append({
        "Model": model_name,
        "Train Accuracy": train_accuracy,
        "Validation Accuracy": validation_accuracy
    })

# Evaluate regression models
regression_results = []
for model_name, model in regression_models.items():
    model.fit(X_train_pca, y_train_reg)
    y_pred_train_reg = model.predict(X_train_pca)
    y_pred_test_reg = model.predict(X_test_pca)

    # Train & Validation MSE & RMSE
    train_mse = mean_squared_error(y_train_reg, y_pred_train_reg)
    train_rmse = np.sqrt(train_mse)
    validation_mse = mean_squared_error(y_test_reg, y_pred_test_reg)
    validation_rmse = np.sqrt(validation_mse)

    regression_results.append({
        "Model": model_name,
        "Train MSE": train_mse,
        "Train RMSE": train_rmse,
        "Validation MSE": validation_mse,
        "Validation RMSE": validation_rmse
    })

# Display results
classification_df = pd.DataFrame(classification_results)
regression_df = pd.DataFrame(regression_results)

print("\nClassification Results (Accuracy):\n", classification_df)
print("\nRegression Results (MSE & RMSE):\n", regression_df)

# Define target and feature matrix
X = df_encoded.drop(columns=["Success_Category", "Recovery_Time"])
y = df_encoded["Success_Category"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardization
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Feature Selection using XGBoost
xgb_feature_selector = xgb.XGBClassifier(n_estimators=50, max_depth=3, reg_lambda=10, subsample=0.7)
xgb_feature_selector.fit(X_train_scaled, y_train)
selector = SelectFromModel(xgb_feature_selector, prefit=True)
X_train_selected = selector.transform(X_train_scaled)
X_test_selected = selector.transform(X_test_scaled)

# PCA for dimensionality reduction
pca = PCA(n_components=0.85)  # Keep 85% variance
X_train_pca = pca.fit_transform(X_train_selected)
X_test_pca = pca.transform(X_test_selected)

# Classification Models with Improved Hyperparameters
classification_models = {
    "Logistic Regression": LogisticRegression(penalty='l1', solver='liblinear', C=0.1, max_iter=1000),
    "SVM (Linear)": SVC(kernel='linear', C=0.1),
    "Random Forest": RandomForestClassifier(n_estimators=50, max_depth=5, min_samples_split=10, min_samples_leaf=5),
    "XGBoost": xgb.XGBClassifier(n_estimators=50, max_depth=3, learning_rate=0.05, reg_lambda=10, subsample=0.7),
    "KNN": KNeighborsClassifier(n_neighbors=9)  # Increase K to reduce variance
}

# Evaluate Models
classification_results = []
for model_name, model in classification_models.items():
    model.fit(X_train_pca, y_train)
    y_pred_train = model.predict(X_train_pca)
    y_pred_test = model.predict(X_test_pca)

    train_accuracy = accuracy_score(y_train, y_pred_train)
    test_accuracy = accuracy_score(y_test, y_pred_test)

    classification_results.append({
        "Model": model_name,
        "Train Accuracy": train_accuracy,
        "Validation Accuracy": test_accuracy
    })

# Display Results
classification_df = pd.DataFrame(classification_results)
print("\nOptimized Classification Results (Accuracy):\n", classification_df)

import matplotlib.pyplot as plt
from sklearn.metrics import log_loss
from sklearn.model_selection import cross_val_score
import joblib

# Define target and feature matrix
X = df_encoded.drop(columns=["Success_Category", "Recovery_Time"])
y = df_encoded["Success_Category"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardization
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Feature Selection using XGBoost
xgb_feature_selector = xgb.XGBClassifier(n_estimators=50, max_depth=3, reg_lambda=10, subsample=0.7)
xgb_feature_selector.fit(X_train_scaled, y_train)
selector = SelectFromModel(xgb_feature_selector, prefit=True)
X_train_selected = selector.transform(X_train_scaled)
X_test_selected = selector.transform(X_test_scaled)

# PCA for dimensionality reduction
pca = PCA(n_components=0.85)  # Keep 85% variance
X_train_pca = pca.fit_transform(X_train_selected)
X_test_pca = pca.transform(X_test_selected)

# Classification Models with Improved Hyperparameters
classification_models = {
    "Logistic Regression": LogisticRegression(penalty='l1', solver='liblinear', C=0.1, max_iter=1000),
    "SVM (Linear)": SVC(kernel='linear', C=0.1, probability=True),  # Enable probability estimation for log_loss
    "Random Forest": RandomForestClassifier(n_estimators=50, max_depth=5, min_samples_split=10, min_samples_leaf=5),
    "XGBoost": xgb.XGBClassifier(n_estimators=50, max_depth=3, learning_rate=0.05, reg_lambda=10, subsample=0.7),
    "KNN": KNeighborsClassifier(n_neighbors=9)  # Increase K to reduce variance
}

# Store losses
train_losses = {model_name: [] for model_name in classification_models}
val_losses = {model_name: [] for model_name in classification_models}

# Evaluate Models with Training and Validation Loss
classification_results = []
best_model = None
best_model_name = ""
best_test_accuracy = 0

for model_name, model in classification_models.items():
    model.fit(X_train_pca, y_train)

    # Predict probabilities for log loss calculation
    if hasattr(model, 'predict_proba'):
        y_train_pred_prob = model.predict_proba(X_train_pca)
        y_test_pred_prob = model.predict_proba(X_test_pca)
    else:
        y_train_pred_prob = model.decision_function(X_train_pca)
        y_test_pred_prob = model.decision_function(X_test_pca)

    # Calculate log loss (training and validation)
    train_loss = log_loss(y_train, y_train_pred_prob)
    val_loss = log_loss(y_test, y_test_pred_prob)

    train_losses[model_name].append(train_loss)
    val_losses[model_name].append(val_loss)

    # Accuracy metrics
    y_pred_train = model.predict(X_train_pca)
    y_pred_test = model.predict(X_test_pca)

    train_accuracy = accuracy_score(y_train, y_pred_train)
    test_accuracy = accuracy_score(y_test, y_pred_test)

    classification_results.append({
        "Model": model_name,
        "Train Accuracy": train_accuracy,
        "Validation Accuracy": test_accuracy,
        "Train Loss": train_loss,
        "Validation Loss": val_loss
    })

    # Save the best model based on validation accuracy
    if test_accuracy > best_test_accuracy:
        best_model = model
        best_model_name = model_name
        best_test_accuracy = test_accuracy

# Display Results
classification_df = pd.DataFrame(classification_results)
print("\nOptimized Classification Results (Accuracy and Loss):\n", classification_df)

# Plot Training and Validation Losses
for model_name in classification_models:
    plt.plot(train_losses[model_name], label=f'{model_name} Train Loss')
    plt.plot(val_losses[model_name], label=f'{model_name} Validation Loss')

plt.title("Training and Validation Losses")
plt.xlabel("Epochs/Iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()

import matplotlib.pyplot as plt
from sklearn.metrics import log_loss
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import RobustScaler
from sklearn.decomposition import PCA
import xgboost as xgb
import joblib

# Define target and feature matrix
X = df_encoded.drop(columns=["Success_Category", "Recovery_Time"])
y = df_encoded["Success_Category"]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Standardization
scaler = RobustScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Feature Selection using XGBoost
xgb_feature_selector = xgb.XGBClassifier(n_estimators=50, max_depth=3, reg_lambda=10, subsample=0.7)
xgb_feature_selector.fit(X_train_scaled, y_train)
selector = SelectFromModel(xgb_feature_selector, prefit=True)
X_train_selected = selector.transform(X_train_scaled)
X_test_selected = selector.transform(X_test_scaled)

# PCA for dimensionality reduction
pca = PCA(n_components=0.85)  # Keep 85% variance
X_train_pca = pca.fit_transform(X_train_selected)
X_test_pca = pca.transform(X_test_selected)

# Classification Models with Improved Hyperparameters
classification_models = {
    "Logistic Regression": LogisticRegression(penalty='l1', solver='liblinear', C=0.1, max_iter=1000),
    "SVM (Linear)": SVC(kernel='linear', C=0.1, probability=True),  # Enable probability estimation for log_loss
    "Random Forest": RandomForestClassifier(n_estimators=50, max_depth=5, min_samples_split=10, min_samples_leaf=5),
    "XGBoost": xgb.XGBClassifier(n_estimators=50, max_depth=3, learning_rate=0.05, reg_lambda=10, subsample=0.7),
    "KNN": KNeighborsClassifier(n_neighbors=9)  # Increase K to reduce variance
}

# Store losses and accuracies
train_losses = {model_name: [] for model_name in classification_models}
val_losses = {model_name: [] for model_name in classification_models}
train_accuracies = {model_name: [] for model_name in classification_models}
val_accuracies = {model_name: [] for model_name in classification_models}

# Evaluate Models with Training and Validation Loss
classification_results = []
best_model = None
best_model_name = ""
best_test_accuracy = 0

for model_name, model in classification_models.items():
    model.fit(X_train_pca, y_train)

    # Predict probabilities for log loss calculation (not all models support this)
    if hasattr(model, 'predict_proba'):
        y_train_pred_prob = model.predict_proba(X_train_pca)
        y_test_pred_prob = model.predict_proba(X_test_pca)
    else:
        y_train_pred_prob = model.decision_function(X_train_pca)
        y_test_pred_prob = model.decision_function(X_test_pca)

    # Calculate log loss (training and validation)
    train_loss = log_loss(y_train, y_train_pred_prob)
    val_loss = log_loss(y_test, y_test_pred_prob)

    train_losses[model_name].append(train_loss)
    val_losses[model_name].append(val_loss)

    # Accuracy metrics
    y_pred_train = model.predict(X_train_pca)
    y_pred_test = model.predict(X_test_pca)

    train_accuracy = accuracy_score(y_train, y_pred_train)
    test_accuracy = accuracy_score(y_test, y_pred_test)

    train_accuracies[model_name].append(train_accuracy)
    val_accuracies[model_name].append(test_accuracy)

    classification_results.append({
        "Model": model_name,
        "Train Accuracy": train_accuracy,
        "Validation Accuracy": test_accuracy,
        "Train Loss": train_loss,
        "Validation Loss": val_loss
    })

    # Save the best model based on validation accuracy
    if test_accuracy > best_test_accuracy:
        best_model = model
        best_model_name = model_name
        best_test_accuracy = test_accuracy

# Display Results
classification_df = pd.DataFrame(classification_results)
print("\nOptimized Classification Results (Accuracy and Loss):\n", classification_df)

# Plot Training and Validation Losses
plt.figure(figsize=(10, 6))
for model_name in classification_models:
    plt.plot(val_losses[model_name], label=f'{model_name} Validation Loss')

plt.title("Validation Losses for Different Models")
plt.xlabel("Model Iterations")
plt.ylabel("Loss")
plt.legend()
plt.grid(True)
plt.show()

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import seaborn as sns
import pandas as pd

# Fit the KNN model
knn_model = classification_models["KNN"]
knn_model.fit(X_train_pca, y_train)

print("Shape of X_train_pca:", X_train_pca.shape)
print("First 5 rows of X_train_pca:\n", X_train_pca[:5])

print("Shape of y_train:", y_train.shape)
print("First 5 labels in y_train:", y_train[:5])

# Predict on test set
y_pred_knn = knn_model.predict(X_test_pca)

# Save the KNN model
joblib.dump(knn_model, "KNN_best_model.pkl")
print("KNN Model saved as KNN_best_model.pkl")

# Evaluate the KNN model

# Accuracy Score
knn_accuracy = accuracy_score(y_test, y_pred_knn)
print(f"KNN Accuracy: {knn_accuracy}")

# Classification Report (Precision, Recall, F1-Score)
knn_classification_report = classification_report(y_test, y_pred_knn)
print("\nKNN Classification Report:\n", knn_classification_report)

# Confusion Matrix
knn_confusion_matrix = confusion_matrix(y_test, y_pred_knn)
print("\nKNN Confusion Matrix:\n", knn_confusion_matrix)

# Plot Confusion Matrix using Seaborn heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(knn_confusion_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=['Class 0', 'Class 1'], yticklabels=['Class 0', 'Class 1'])
plt.title("KNN Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("True")
plt.show()

from sklearn.linear_model import ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.metrics import r2_score

# New regression models
regression_models = {
    "ElasticNet": ElasticNet(alpha=0.5, l1_ratio=0.5),  # Combination of Lasso and Ridge
    "Decision Tree Regressor": DecisionTreeRegressor(max_depth=5),  # Simpler model
    "AdaBoost Regressor": AdaBoostRegressor(n_estimators=100),  # Boosting model
    "Random Forest Regressor": RandomForestRegressor(n_estimators=50, max_depth=5),
    "Linear Regression": Ridge(alpha=100),
    "KNN Regressor": KNeighborsRegressor(n_neighbors=7),
    "XGBoost Regressor": xgb.XGBRegressor(n_estimators=50, max_depth=3, learning_rate=0.05, subsample=0.7, gamma=5),
}

# Evaluate regression models
regression_results = []
for model_name, model in regression_models.items():
    model.fit(X_train_pca, y_train_reg)
    y_pred_train_reg = model.predict(X_train_pca)
    y_pred_test_reg = model.predict(X_test_pca)

    # Train & Validation MSE, RMSE, and R-squared
    train_mse = mean_squared_error(y_train_reg, y_pred_train_reg)
    train_rmse = np.sqrt(train_mse)
    validation_mse = mean_squared_error(y_test_reg, y_pred_test_reg)
    validation_rmse = np.sqrt(validation_mse)

    # Calculate R-squared for better understanding of the model performance
    train_r2 = r2_score(y_train_reg, y_pred_train_reg)
    validation_r2 = r2_score(y_test_reg, y_pred_test_reg)

    regression_results.append({
        "Model": model_name,
        "Train MSE": train_mse,
        "Train RMSE": train_rmse,
        "Train R^2": train_r2,
        "Validation MSE": validation_mse,
        "Validation RMSE": validation_rmse,
        "Validation R^2": validation_r2
    })

print("Original X_train shape:", X_train.shape if 'X_train' in locals() else "Not Found")
print("Transformed X_train (X_train_pca) shape:", X_train_pca.shape if 'X_train_pca' in locals() else "Not Found")

print("Original y_train shape:", y_train_reg.shape if 'y_train_reg' in locals() else "Not Found")
print("Original y_test shape:", y_test_reg.shape if 'y_test_reg' in locals() else "Not Found")

# Display results
regression_df = pd.DataFrame(regression_results)

print("\nModified Regression Results (MSE, RMSE, and R^2):\n", regression_df)

# Define model save directory
model_save_path = "RandomForestRegressor.pkl"

# Save the Random Forest Regressor
joblib.dump(regression_models["Random Forest Regressor"], model_save_path)

print(f"Random Forest Regressor model saved at: {model_save_path}")

from sklearn.model_selection import GridSearchCV

# Hyperparameter tuning for models

dt_params = {
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'criterion': ['squared_error', 'absolute_error', 'friedman_mse']  # Valid values
}


# Random Forest Regressor tuning
rf_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [5, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# XGBoost Regressor tuning
xgb_params = {
    'n_estimators': [50, 100, 200],
    'learning_rate': [0.01, 0.05, 0.1],
    'max_depth': [3, 5, 7],
    'subsample': [0.7, 0.8, 0.9],
    'gamma': [0, 1, 5],
    'lambda': [1, 5, 10],
    'min_child_weight': [1, 3, 5]
}

# ElasticNet tuning
en_params = {
    'alpha': [0.1, 0.5, 1, 10],
    'l1_ratio': [0.1, 0.5, 0.9]
}

# GridSearchCV with cross-validation
dt_grid = GridSearchCV(DecisionTreeRegressor(), dt_params, cv=5, n_jobs=-1, verbose=1)
rf_grid = GridSearchCV(RandomForestRegressor(), rf_params, cv=5, n_jobs=-1, verbose=1)
xgb_grid = GridSearchCV(xgb.XGBRegressor(), xgb_params, cv=5, n_jobs=-1, verbose=1)
en_grid = GridSearchCV(ElasticNet(), en_params, cv=5, n_jobs=-1, verbose=1)

# Fit the models with the best parameters
dt_grid.fit(X_train_pca, y_train_reg)
rf_grid.fit(X_train_pca, y_train_reg)
xgb_grid.fit(X_train_pca, y_train_reg)
en_grid.fit(X_train_pca, y_train_reg)

# Get best models after tuning
best_dt = dt_grid.best_estimator_
best_rf = rf_grid.best_estimator_
best_xgb = xgb_grid.best_estimator_
best_en = en_grid.best_estimator_

# Evaluate the tuned models
tuned_models = {
    "Decision Tree": best_dt,
    "Random Forest": best_rf,
    "XGBoost": best_xgb,
    "ElasticNet": best_en
}

# Evaluate the tuned models
tuned_regression_results = []
for model_name, model in tuned_models.items():
    model.fit(X_train_pca, y_train_reg)
    y_pred_train_reg = model.predict(X_train_pca)
    y_pred_test_reg = model.predict(X_test_pca)

    # Train & Validation MSE, RMSE, and R-squared
    train_mse = mean_squared_error(y_train_reg, y_pred_train_reg)
    train_rmse = np.sqrt(train_mse)
    validation_mse = mean_squared_error(y_test_reg, y_pred_test_reg)
    validation_rmse = np.sqrt(validation_mse)

    train_r2 = r2_score(y_train_reg, y_pred_train_reg)
    validation_r2 = r2_score(y_test_reg, y_pred_test_reg)

    tuned_regression_results.append({
        "Model": model_name,
        "Train MSE": train_mse,
        "Train RMSE": train_rmse,
        "Train R^2": train_r2,
        "Validation MSE": validation_mse,
        "Validation RMSE": validation_rmse,
        "Validation R^2": validation_r2
    })

# Display the tuned results
tuned_regression_df = pd.DataFrame(tuned_regression_results)
print("\nTuned Regression Results (MSE, RMSE, and R^2):\n", tuned_regression_df)

from sklearn.model_selection import GridSearchCV
from sklearn.ensemble import RandomForestRegressor
import xgboost as xgb

# Define parameter grid for Random Forest
rf_params = {
    'n_estimators': [100, 300, 500],
    'max_depth': [5, 7, 10],
    'min_samples_split': [5, 10, 20],
    'min_samples_leaf': [3, 5, 10],
    'max_features': ['sqrt', 'log2']
}

# Define parameter grid for XGBoost
xgb_params = {
    'n_estimators': [300, 500],
    'learning_rate': [0.01, 0.05],
    'max_depth': [3, 6, 10],
    'subsample': [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.9],
    'reg_lambda': [10, 50]
}

# Perform Grid Search for Random Forest
rf_grid = GridSearchCV(
    RandomForestRegressor(random_state=42),
    rf_params,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    verbose=2
)
rf_grid.fit(X_train_pca, y_train_reg)
best_rf = rf_grid.best_estimator_

# Perform Grid Search for XGBoost
xgb_grid = GridSearchCV(
    xgb.XGBRegressor(objective='reg:squarederror', random_state=42),
    xgb_params,
    cv=5,
    scoring='neg_mean_squared_error',
    n_jobs=-1,
    verbose=2
)
xgb_grid.fit(X_train_pca, y_train_reg)
best_xgb = xgb_grid.best_estimator_

# Evaluate the models
def evaluate_model(model, X_train, y_train, X_test, y_test):
    y_train_pred = model.predict(X_train)
    y_test_pred = model.predict(X_test)

    train_mse = mean_squared_error(y_train, y_train_pred)
    train_rmse = np.sqrt(train_mse)
    train_r2 = model.score(X_train, y_train)

    test_mse = mean_squared_error(y_test, y_test_pred)
    test_rmse = np.sqrt(test_mse)
    test_r2 = model.score(X_test, y_test)

    return {
        "Train MSE": train_mse,
        "Train RMSE": train_rmse,
        "Train R²": train_r2,
        "Test MSE": test_mse,
        "Test RMSE": test_rmse,
        "Test R²": test_r2
    }

rf_results = evaluate_model(best_rf, X_train_pca, y_train_reg, X_test_pca, y_test_reg)
xgb_results = evaluate_model(best_xgb, X_train_pca, y_train_reg, X_test_pca, y_test_reg)

# Display results
print("\nRandom Forest Results:", rf_results)
print("\nXGBoost Results:", xgb_results)

# After feature engineering in your original code:
import joblib

# Save all critical preprocessing objects
joblib.dump(vectorizer, 'tfidf_vectorizer.pkl')  # Renamed to match variable in original code
joblib.dump(disease_encoder, 'disease_encoder.pkl')
joblib.dump(treatment_encoder, 'treatment_encoder.pkl')
joblib.dump(poly, 'poly_features.pkl')  # Save PolynomialFeatures
joblib.dump(scaler, 'robust_scaler.pkl')
joblib.dump(selector, 'feature_selector.pkl')
joblib.dump(pca, 'pca_transformer.pkl')

